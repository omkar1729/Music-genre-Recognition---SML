{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abantika Mel Spectrogram CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbBoRKO_unb3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "f13013e8-e050-4622-940a-4e94e3635fff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foL6rV2Iu3gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_directory = '/content/gdrive/My Drive/train1'\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c9p8raqyvmqL",
        "colab": {}
      },
      "source": [
        "import os\n",
        "def getcategories(categories,list_of_files,path):\n",
        "    for files in os.listdir(path):\n",
        "        if \".mf\" not in files:\n",
        "            categories.append(files)\n",
        "            list_of_files[files] = list()\n",
        "            newPath = os.path.join(path,files)\n",
        "            for f in os.listdir(newPath):\n",
        "                list_of_files[files].append(f)\n",
        "    #print(list_of_files)\n",
        "    return(categories,list_of_files)\n",
        "\n",
        "\n",
        "categories=[] # a list of all categories ['pop','rock',.....,'jazz']\n",
        "list_of_files = {} # a dictionary with the genre as the key value. For every key value, we have a list which contains the names of the audio files contained within that genre folder\n",
        "categories,list_of_files = getcategories(categories,list_of_files,train_directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_XRDxl44WE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "def get_index(category,categories):\n",
        "    print(category)\n",
        "    for i in range(0,len(categories)):\n",
        "        if(categories[i]==category):\n",
        "            break\n",
        "    return(i)\n",
        "\n",
        "def genTrainTest(training_data,test_data,list_of_files,categories,directory):\n",
        "    list1 = [x for x in range(100)] #Since we have 100 files in each genre, I take a list which contains values like [0,1,2,3,4,.....,100].\n",
        "    list2 = list1 #create a backup of list1\n",
        "    for category in list_of_files.keys(): #For each music genre\n",
        "        category1 = category + \"/\"\n",
        "        path = os.path.join(directory,category1) #helps to navigate through the required genre directory\n",
        "        #print(path)\n",
        "        class_label = get_index(category,categories) #we have to represent each category by a number which is nothing but the position of the genre name in the categories list. \n",
        "        test_index=[]\n",
        "        for direc,_,filenames in os.walk(path):\n",
        "              #print(\"Length of filenames\"+str(len(filenames)))\n",
        "             # print(filenames)\n",
        "              random.shuffle(list1) #shuffling the list \n",
        "              train_index = list1[:70] #take the first 70 random indices as train indices\n",
        "              #print(\"List1: \"+str(list1))\n",
        "              #print(\"Training index: \"+str(train_index))\n",
        "              #print(len(train_index))\n",
        "              #count=0\n",
        "              #count1 = 0\n",
        "              #count2=0\n",
        "              #print(len(list1))\n",
        "              for i in range(0,len(list1)):\n",
        "                if(list1[i] not in train_index):\n",
        "                    test_index.append(list1[i]) #remaining 30 for test indices\n",
        "              #print(count,count1,count2)\n",
        "              #print(train1_index==train_index)\n",
        "              #test_index = list(set(list1)-set(train_index))\n",
        "              #print(len(test_index))\n",
        "              #print(\"Test index: \"+str(test_index))\n",
        "\n",
        "\n",
        "              for i in train_index:\n",
        "                    full_path=os.path.join(path,filenames[i]) #navigate to each genre subfolder\n",
        "                    img_arr = cv2.imread(full_path) #Read the image\n",
        "                    #print(img_arr)\n",
        "                    im_rgb = cv2.cvtColor(img_arr,cv2.COLOR_BGR2RGB) #Convert into RGB as colour is important\n",
        "                    #print(im_rgb.shape)\n",
        "\n",
        "\n",
        "                    #plt.imshow(im_rgb)\n",
        "                    #plt.show()\n",
        "                    training_data.append([im_rgb,class_label]) #each row in the training data contains the rgb numpy matrix and the class label \n",
        "\n",
        "\n",
        "\n",
        "              for i in test_index:\n",
        "                    full_path=os.path.join(path,filenames[i])\n",
        "                    img_arr = cv2.imread(full_path)\n",
        "                    #print(img_arr.shape)\n",
        "                    im_rgb = cv2.cvtColor(img_arr,cv2.COLOR_BGR2RGB)\n",
        "                    #print(im_rgb.shape)\n",
        "                    #plt.imshow(im_rgb)\n",
        "                    #plt.show()\n",
        "                    test_data.append([im_rgb,class_label]) #similarly for test data\n",
        "                    \n",
        "                    \n",
        "    return training_data,test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVzbpfFt4-LO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_numpy(training_data,test_data):\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for features,class_label in training_data:\n",
        "        X.append(features)\n",
        "        Y.append(class_label)\n",
        "        \n",
        "    #X has to be a numpy array\n",
        "    Y=np.array(Y)\n",
        "    #X=np.asarray(X)\n",
        "    #print(len(X[0]))\n",
        "    #print(len(X[0][0]))\n",
        "    #print(len(X[0][0][0]))\n",
        "    #X=np.array(X)\n",
        "    X=np.array(X)\n",
        "    #print(X.shape)\n",
        "    #print(Y.shape)\n",
        "    np.save(\"X.npy\",X)\n",
        "    np.save(\"Y.npy\",Y)\n",
        "\n",
        "\n",
        "    X2 = []\n",
        "    Y2 = []\n",
        "    for features, class_label in test_data:\n",
        "        X2.append(features)\n",
        "        Y2.append(class_label)\n",
        "    X2 = np.array(X2)\n",
        "    Y2 = np.array(Y2)\n",
        "    np.save(\"X2.npy\",X2)\n",
        "    np.save(\"Y2.npy\",Y2)\n",
        "\n",
        "\n",
        "    return(X,Y,X2,Y2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBHs33zV6SGS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc65209f-2fa4-49fc-bab6-55990d6a2cde"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQRT_Nq05DuQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "1c28f82d-9d9a-4e8a-fa24-930499e88f8e"
      },
      "source": [
        "\n",
        "\n",
        "training_data=[]\n",
        "test_data=[]\n",
        "training_data, test_data = genTrainTest(training_data,test_data,list_of_files,categories,train_directory)\n",
        "print(len(training_data))\n",
        "print(len(test_data))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metal\n",
            "country\n",
            "pop\n",
            "blues\n",
            "reggae\n",
            "jazz\n",
            "disco\n",
            "hiphop\n",
            "rock\n",
            "classical\n",
            "700\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK85S8lu5OPu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20843fe3-dd9b-448a-de74-ada26643fbf0"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential,Model\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, LSTM,Conv2D, Dropout,Input, Flatten,Activation,MaxPooling2D,AveragePooling2D,BatchNormalization\n",
        "def convert_to_numpy(training_data,test_data):\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for features,class_label in training_data:\n",
        "        X.append(features)\n",
        "        Y.append(class_label)\n",
        "        \n",
        "    #X has to be a numpy array\n",
        "    Y=np.array(Y)\n",
        "    #X=np.asarray(X)\n",
        "    #print(len(X[0]))\n",
        "    #print(len(X[0][0]))\n",
        "    #print(len(X[0][0][0]))\n",
        "    #X=np.array(X)\n",
        "    X=np.array(X)\n",
        "    #print(X.shape)\n",
        "    #print(Y.shape)\n",
        "    np.save(\"X.npy\",X)\n",
        "    np.save(\"Y.npy\",Y)\n",
        "\n",
        "\n",
        "    X2 = []\n",
        "    Y2 = []\n",
        "    for features, class_label in test_data:\n",
        "        X2.append(features)\n",
        "        Y2.append(class_label)\n",
        "    X2 = np.array(X2)\n",
        "    Y2 = np.array(Y2)\n",
        "    np.save(\"X2.npy\",X2)\n",
        "    np.save(\"Y2.npy\",Y2)\n",
        "\n",
        "\n",
        "    return(X,Y,X2,Y2)\n",
        "\n",
        "def trainCNNLSTM():\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(32,(3,3),padding='same',activation='relu',input_shape=(480,640,3))))\n",
        "    model.add(TimeDistributed(Conv2D(32,(3,3),activation='relu')))\n",
        "    model.add(TimeDistributed(AveragePooling2D(pool_size = (2,2))))\n",
        "    #model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(TimeDistributed(Conv2D(64,(4,4),padding='same',activation='relu',strides=(2,2))))\n",
        "    model.add(TimeDistributed(Conv2D(64,(4,4),activation='relu',strides=(2,2))))\n",
        "    model.add(TimeDistributed(AveragePooling2D(pool_size = (2,2))))\n",
        "    #model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(TimeDistributed(Conv2D(64,(4,4),padding='same',activation='relu')))\n",
        "    model.add(TimeDistributed(Conv2D(64,(4,4),activation='relu')))\n",
        "    model.add(TimeDistributed(AveragePooling2D(pool_size = (2,2))))\n",
        "    #model.add(Dropout(0.25))\n",
        "    #model.add(Conv2D(64,(10,10),padding='same',activation='relu',strides=(2,2)))\n",
        "    #model.add(Conv2D(64,(10,10),activation='relu',strides=(2,2)))\n",
        "    #model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "    #model.add(Dropout(0.25))\n",
        "\n",
        "    #LSTM\n",
        "\n",
        "\n",
        "    #\n",
        "    \"\"\"\n",
        "    lstm1,h,c = LSTM(96, return_sequences=True,return_state=True)([lstm1,h,c])\n",
        "    lstm1,h,c = LSTM(96, return_sequences=False,return_state=True)([lstm1,h,c])\n",
        "    #y = Lambda(lambda x: tf.keras.backend.concatenate([h,c],0))([lstm1,h,c])\n",
        "    y = Concatenate()([h,c])\n",
        "    model_language = Model(inputs=inputs1, outputs=y)\n",
        "    # combined model\n",
        "    conc = keras.layers.Multiply()([model_language.output,model.output])\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(96,stateful=True,return_sequences=True))\n",
        "    model.add(LSTM(50,stateful=True))\n",
        "\n",
        "    #model.add(Dense(1000,activation='relu'))\n",
        "    #model.add(Dropout(0.3))\n",
        "    #model.add(Dense(512,activation='relu'))\n",
        "    #model.add(Dropout(0.5))\n",
        "    #model.add(Dense(64,activation='relu'))\n",
        "\n",
        "    model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    return(model)\n",
        "\n",
        "def train_model():\n",
        "\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32,(3,3),padding='same',activation='relu',input_shape=(480,640,3)))\n",
        "    model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "    model.add(AveragePooling2D(pool_size = (2,2)))\n",
        "    #model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64,(4,4),padding='same',activation='relu',strides=(2,2)))\n",
        "    model.add(Conv2D(64,(4,4),activation='relu',strides=(2,2)))\n",
        "    model.add(AveragePooling2D(pool_size = (2,2)))\n",
        "    #model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64,(4,4),padding='same',activation='relu'))\n",
        "    model.add(Conv2D(64,(4,4),activation='relu'))\n",
        "    model.add(AveragePooling2D(pool_size = (2,2)))\n",
        "    #model.add(Dropout(0.25))\n",
        "    #model.add(Conv2D(64,(10,10),padding='same',activation='relu',strides=(2,2)))\n",
        "    #model.add(Conv2D(64,(10,10),activation='relu',strides=(2,2)))\n",
        "    #model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "    #model.add(Dropout(0.25))\n",
        "\n",
        "    #LSTM\n",
        "\n",
        "\n",
        "    #model.add(LSTM(96,input_shape=(7,704)))\n",
        "    \"\"\"\n",
        "    lstm1,h,c = LSTM(96, return_sequences=True,return_state=True)([lstm1,h,c])\n",
        "    lstm1,h,c = LSTM(96, return_sequences=False,return_state=True)([lstm1,h,c])\n",
        "    #y = Lambda(lambda x: tf.keras.backend.concatenate([h,c],0))([lstm1,h,c])\n",
        "    y = Concatenate()([h,c])\n",
        "    model_language = Model(inputs=inputs1, outputs=y)\n",
        "    # combined model\n",
        "    conc = keras.layers.Multiply()([model_language.output,model.output])\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000,activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(512,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64,activation='relu'))\n",
        "\n",
        "    model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    return(model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X,Y,X2,Y2 = convert_to_numpy(training_data,test_data) #since training_data and test_data are lists, we need to convert them into numpy arrays for further processing.\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(X2.shape)\n",
        "print(Y2.shape)\n",
        "\n",
        "X = X.astype('float32') #Convert all the pixel values to float type as we are going to normalise them for training features\n",
        "X2 = X2.astype('float32') #Convert all the pixel values to float type as we are going to normalise them for test features\n",
        "X/=255 #Normalising the training features\n",
        "X2/=255 #Normalising the test features\n",
        "\n",
        "Y1=to_categorical(Y,10) #use one-hot encoding for training class labels (required for cnn)\n",
        "Y3 = to_categorical(Y2,10) #use one-hot encoding for test class labels (required for cnn)\n",
        "\n",
        "model = train_model()\n",
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(X,Y1,batch_size =32 ,epochs=30,verbose=1,validation_data=(X2,Y3))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(700, 480, 640, 3)\n",
            "(700,)\n",
            "(300, 480, 640, 3)\n",
            "(300,)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 480, 640, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 478, 638, 32)      9248      \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 239, 319, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 120, 160, 64)      32832     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 59, 79, 64)        65600     \n",
            "_________________________________________________________________\n",
            "average_pooling2d_2 (Average (None, 29, 39, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 29, 39, 64)        65600     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 26, 36, 64)        65600     \n",
            "_________________________________________________________________\n",
            "average_pooling2d_3 (Average (None, 13, 18, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 14976)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1000)              14977000  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               512512    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 15,762,770\n",
            "Trainable params: 15,762,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 700 samples, validate on 300 samples\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "700/700 [==============================] - 38s 55ms/step - loss: 2.3927 - acc: 0.1000 - val_loss: 2.2998 - val_acc: 0.1633\n",
            "Epoch 2/30\n",
            "700/700 [==============================] - 24s 35ms/step - loss: 2.3934 - acc: 0.1286 - val_loss: 2.2614 - val_acc: 0.1000\n",
            "Epoch 3/30\n",
            "700/700 [==============================] - 24s 35ms/step - loss: 2.2496 - acc: 0.1100 - val_loss: 2.1619 - val_acc: 0.2000\n",
            "Epoch 4/30\n",
            "700/700 [==============================] - 24s 35ms/step - loss: 2.2730 - acc: 0.1557 - val_loss: 2.0829 - val_acc: 0.2533\n",
            "Epoch 5/30\n",
            "700/700 [==============================] - 24s 35ms/step - loss: 2.1484 - acc: 0.1943 - val_loss: 1.9867 - val_acc: 0.2533\n",
            "Epoch 6/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 2.1499 - acc: 0.1886 - val_loss: 2.1291 - val_acc: 0.1800\n",
            "Epoch 7/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 2.1558 - acc: 0.2300 - val_loss: 1.9719 - val_acc: 0.2900\n",
            "Epoch 8/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 2.0324 - acc: 0.2771 - val_loss: 1.8935 - val_acc: 0.2933\n",
            "Epoch 9/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.9483 - acc: 0.3000 - val_loss: 1.9981 - val_acc: 0.2500\n",
            "Epoch 10/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.9258 - acc: 0.3057 - val_loss: 1.7341 - val_acc: 0.3600\n",
            "Epoch 11/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.8422 - acc: 0.3171 - val_loss: 1.5903 - val_acc: 0.4067\n",
            "Epoch 12/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.8595 - acc: 0.3371 - val_loss: 1.6185 - val_acc: 0.4100\n",
            "Epoch 13/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.7306 - acc: 0.3971 - val_loss: 1.5252 - val_acc: 0.4333\n",
            "Epoch 14/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.5779 - acc: 0.4343 - val_loss: 1.8260 - val_acc: 0.3867\n",
            "Epoch 15/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.5847 - acc: 0.4700 - val_loss: 1.5218 - val_acc: 0.4767\n",
            "Epoch 16/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.3327 - acc: 0.5400 - val_loss: 1.6972 - val_acc: 0.4700\n",
            "Epoch 17/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.3010 - acc: 0.5429 - val_loss: 1.4110 - val_acc: 0.5300\n",
            "Epoch 18/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.1621 - acc: 0.5986 - val_loss: 1.4245 - val_acc: 0.5100\n",
            "Epoch 19/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.1448 - acc: 0.6243 - val_loss: 2.2315 - val_acc: 0.4167\n",
            "Epoch 20/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.0309 - acc: 0.6586 - val_loss: 1.0722 - val_acc: 0.6400\n",
            "Epoch 21/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.9354 - acc: 0.6814 - val_loss: 1.3110 - val_acc: 0.6033\n",
            "Epoch 22/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.8344 - acc: 0.7200 - val_loss: 1.3536 - val_acc: 0.5933\n",
            "Epoch 23/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.7069 - acc: 0.7671 - val_loss: 1.5920 - val_acc: 0.5933\n",
            "Epoch 24/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.7420 - acc: 0.7400 - val_loss: 1.2266 - val_acc: 0.6433\n",
            "Epoch 25/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.5454 - acc: 0.8229 - val_loss: 1.7898 - val_acc: 0.5733\n",
            "Epoch 26/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.5014 - acc: 0.8500 - val_loss: 2.0931 - val_acc: 0.5767\n",
            "Epoch 27/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.4128 - acc: 0.8614 - val_loss: 1.7531 - val_acc: 0.5900\n",
            "Epoch 28/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.3671 - acc: 0.8843 - val_loss: 1.6996 - val_acc: 0.5333\n",
            "Epoch 29/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.3171 - acc: 0.9000 - val_loss: 1.9474 - val_acc: 0.6167\n",
            "Epoch 30/30\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.3446 - acc: 0.8986 - val_loss: 1.7892 - val_acc: 0.6400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e51863080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5dc2Bjl-AYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "4ab55efa-80b8-4fcf-d6c5-b658cf0b427f"
      },
      "source": [
        "model.fit(X,Y1,batch_size =32 ,epochs=10,verbose=1,validation_data=(X2,Y3))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 700 samples, validate on 300 samples\n",
            "Epoch 1/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.4065 - acc: 0.9000 - val_loss: 1.6699 - val_acc: 0.6300\n",
            "Epoch 2/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.1908 - acc: 0.9514 - val_loss: 1.7895 - val_acc: 0.6467\n",
            "Epoch 3/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.2537 - acc: 0.9343 - val_loss: 2.0245 - val_acc: 0.5933\n",
            "Epoch 4/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.3603 - acc: 0.9386 - val_loss: 1.8272 - val_acc: 0.6100\n",
            "Epoch 5/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.0973 - acc: 0.9729 - val_loss: 2.4356 - val_acc: 0.5567\n",
            "Epoch 6/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.1153 - acc: 0.9629 - val_loss: 2.1565 - val_acc: 0.6667\n",
            "Epoch 7/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.1219 - acc: 0.9714 - val_loss: 2.7090 - val_acc: 0.6133\n",
            "Epoch 8/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.3330 - acc: 0.9429 - val_loss: 2.6174 - val_acc: 0.6100\n",
            "Epoch 9/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.0750 - acc: 0.9871 - val_loss: 3.1866 - val_acc: 0.5467\n",
            "Epoch 10/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.1562 - acc: 0.9614 - val_loss: 2.9396 - val_acc: 0.5600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7eb38fb518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvf1t1LDBofu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "ec5620e6-985d-4bf8-cb04-63d971d4386b"
      },
      "source": [
        "model.fit(X,Y1,batch_size =32 ,epochs=10,verbose=1,validation_data=(X2,Y3))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 700 samples, validate on 300 samples\n",
            "Epoch 1/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.1569 - acc: 0.9543 - val_loss: 2.6019 - val_acc: 0.5967\n",
            "Epoch 2/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.1084 - acc: 0.9657 - val_loss: 2.9758 - val_acc: 0.5400\n",
            "Epoch 3/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.1174 - acc: 0.9614 - val_loss: 2.4066 - val_acc: 0.6100\n",
            "Epoch 4/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.0730 - acc: 0.9786 - val_loss: 3.1348 - val_acc: 0.6000\n",
            "Epoch 5/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.2320 - acc: 0.9543 - val_loss: 2.3938 - val_acc: 0.6033\n",
            "Epoch 6/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.0067 - acc: 0.9986 - val_loss: 2.7908 - val_acc: 0.6333\n",
            "Epoch 7/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 6.8036e-04 - acc: 1.0000 - val_loss: 3.1410 - val_acc: 0.6400\n",
            "Epoch 8/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 1.4100e-04 - acc: 1.0000 - val_loss: 3.6003 - val_acc: 0.6200\n",
            "Epoch 9/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.5474 - acc: 0.9500 - val_loss: 3.8731 - val_acc: 0.5433\n",
            "Epoch 10/10\n",
            "700/700 [==============================] - 25s 35ms/step - loss: 0.0554 - acc: 0.9857 - val_loss: 3.9736 - val_acc: 0.5667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7eb70b7198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlR5ceNODgp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}